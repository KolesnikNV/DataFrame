# Работа с данными с использованием Apache Spark
Этот репозиторий содержит пример Python-кода для работы с данными с использованием Apache Spark. Код демонстрирует создание сессии Spark, создание датафреймов, выполнение SQL-запросов и обработку данных с помощью библиотеки PySpark.

# Требования
Прежде чем начать работу с этим кодом, убедитесь, что на вашей машине установлены следующие компоненты:

Python 3
Apache Spark 
PySpark 

Проверьте текущую версию Java (если она установлена):

```java -version```

Если Java уже установлена, это покажет текущую версию. Если Java не установлена, вы увидите сообщение о том, что Java не найдена.

Загрузите и установите Java Development Kit (JDK):

Посетите официальный сайт Oracle или OpenJDK (проект с открытым исходным кодом) и загрузите соответствующую версию JDK для вашей операционной системы. Обычно рекомендуется использовать последнюю стабильную версию.

Установите JDK:

В зависимости от вашей операционной системы и варианта JDK (Oracle JDK или OpenJDK), установочные шаги могут различаться. Обычно вы должны будете запустить установщик и следовать инструкциям по установке.

Настройте переменные окружения (опционально):

В некоторых случаях вам может потребоваться настроить переменные окружения JAVA_HOME и PATH, чтобы указать системе, где находится JDK. Это может понадобиться для того, чтобы другие программы и среды могли найти Java.

Для Linux и macOS, вы можете добавить следующие строки в ваш ~/.bashrc или ~/.zshrc файл (замените /путь/к/вашей/jdk на путь к вашей установленной JDK):

```export JAVA_HOME=/путь/к/вашей/jdk```
```export PATH=$PATH:$JAVA_HOME/bin```

Для Windows, вы можете добавить JAVA_HOME и %JAVA_HOME%\bin в переменные среды.

Проверьте установку:

После установки выполните команду ```java -version``` ещё раз в терминале. Теперь вы должны увидеть версию установленной Java.
# Установка
Клонируйте репозиторий на свой компьютер:

```git clone https://github.com/KolesnikNV/DataFrame.git```
Перейдите в каталог проекта:

Услановите зависимости через poetry или pip
# Использование
Запустите основной скрипт для обработки данных:

```python main.py```

Этот скрипт создаст сессию Spark, создаст датафреймы для продуктов и категорий, выполнит SQL-запрос для объединения данных и выведет результаты.